{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "MAX_LENGTH = 25\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "CLIP = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go</td>\n",
       "      <td>ve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>go</td>\n",
       "      <td>vete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>go</td>\n",
       "      <td>vaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>go</td>\n",
       "      <td>vayase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>hi</td>\n",
       "      <td>hola</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 English words/sentences French words/sentences\n",
       "0           0                      go                     ve\n",
       "1           1                      go                   vete\n",
       "2           2                      go                   vaya\n",
       "3           3                      go                 vayase\n",
       "4           4                      hi                   hola"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"eng_spn.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = df[\"English words/sentences\"]\n",
    "target_data = df[\"French words/sentences\"].apply(lambda x: \"<sos> \" + x + \" <eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(input_data)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_data)\n",
    "\n",
    "target_tokenizer = Tokenizer()\n",
    "target_tokenizer.fit_on_texts(target_data)\n",
    "target_sequences = target_tokenizer.texts_to_sequences(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "padded_input_sequences = pad_sequences(\n",
    "    input_sequences, maxlen=MAX_LENGTH, padding=\"post\"\n",
    ")\n",
    "\n",
    "padded_target_sequences = pad_sequences(\n",
    "    target_sequences, maxlen=MAX_LENGTH, padding=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab size\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pytorch tensors\n",
    "input_tensor = torch.tensor(padded_input_sequences, dtype=torch.long)\n",
    "target_tensor = torch.tensor(padded_target_sequences, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dataloader = DataLoader(\n",
    "    TensorDataset(input_tensor, target_tensor), batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert tokens to vectors\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Pass the embedded vector into LSTM layer\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Linear layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # Add batch dimension\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # Convert tokens to vectors\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Pass the embedded vector into LSTM layer\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "\n",
    "        # Remove the batch dimension\n",
    "        lstm_output = lstm_output.squeeze(1)\n",
    "\n",
    "        # Generate predictions for the next token\n",
    "        prediction = self.fc(lstm_output)\n",
    "\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_vocab_size,\n",
    "        target_vocab_size,\n",
    "        embed_dim,\n",
    "        hidden_dim,\n",
    "        num_layers,\n",
    "        dropout,\n",
    "    ):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            input_vocab_size, embed_dim, hidden_dim, num_layers, dropout\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            target_vocab_size, embed_dim, hidden_dim, num_layers, dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        batch_size, max_length = target.size()\n",
    "        target_vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        # Tensor to store outputs for all time steps\n",
    "        outputs = torch.zeros(batch_size, max_length, target_vocab_size)\n",
    "\n",
    "        # Get hidden and cell states from the encoder\n",
    "        hidden, cell = self.encoder(input)\n",
    "\n",
    "        # Start decoding with the first target token\n",
    "        target_input_token = target[:, 0]\n",
    "\n",
    "        for t in range(1, max_length):\n",
    "            decoder_output, hidden, cell = self.decoder(\n",
    "                target_input_token, hidden, cell\n",
    "            )\n",
    "            outputs[:, t, :] = decoder_output\n",
    "            target_input_token = target[:, t]\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Seq2Seq(\n",
    "    input_vocab_size,\n",
    "    target_vocab_size,\n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_LAYERS,\n",
    "    DROPOUT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "\n",
    "\n",
    "# Save model function\n",
    "def save_checkpoint(epoch, model, filename=\"checkpoint.pth\"):\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "        },\n",
    "        filename,\n",
    "    )\n",
    "\n",
    "\n",
    "# Load model function\n",
    "def load_checkpoint(model, filename):\n",
    "    checkpoint = torch.load(filename)\n",
    "\n",
    "    start_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    return start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, starting training from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aks7d\\AppData\\Local\\Temp\\ipykernel_9656\\3896224334.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename)\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "try:\n",
    "    start_epoch = load_checkpoint(model, filename=\"checkpoint.pth\")\n",
    "    print(f\"Resuming training from epoch: {start_epoch}\")\n",
    "except FileNotFoundError:\n",
    "    start_epoch = 1\n",
    "    print(f\"No checkpoint found, starting training from scratch...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Adam optimizer and Loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "def train(model, optimizer, criterion, dataloader, epochs=NUM_EPOCHS):\n",
    "\n",
    "    model.train()  # Set model to Training mode\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "        for input, target in progress_bar:\n",
    "\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(input, target)\n",
    "\n",
    "            # Reshape input and target to calculate loss\n",
    "            output = output[:, 1:].reshape(-1, output.shape[2])\n",
    "            target = target[:, 1:].reshape(-1)\n",
    "\n",
    "            # Compute loss and backpropagation\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "\n",
    "            # Update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        total_loss += epoch_loss\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "        save_checkpoint(epoch, model)\n",
    "\n",
    "    print(f\"Total Loss: {total_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 157/157 [02:56<00:00,  1.13s/it, loss=4.76]\n",
      "Epoch 2/50: 100%|██████████| 157/157 [02:53<00:00,  1.11s/it, loss=4.42]\n",
      "Epoch 3/50: 100%|██████████| 157/157 [02:51<00:00,  1.09s/it, loss=4.08]\n",
      "Epoch 4/50: 100%|██████████| 157/157 [03:05<00:00,  1.18s/it, loss=4.09]\n",
      "Epoch 5/50: 100%|██████████| 157/157 [03:05<00:00,  1.18s/it, loss=3.94]\n",
      "Epoch 6/50: 100%|██████████| 157/157 [03:18<00:00,  1.27s/it, loss=3.07]\n",
      "Epoch 7/50: 100%|██████████| 157/157 [03:33<00:00,  1.36s/it, loss=2.96]\n",
      "Epoch 8/50: 100%|██████████| 157/157 [03:15<00:00,  1.24s/it, loss=2.97]\n",
      "Epoch 9/50: 100%|██████████| 157/157 [03:16<00:00,  1.25s/it, loss=2.66]\n",
      "Epoch 10/50: 100%|██████████| 157/157 [02:55<00:00,  1.12s/it, loss=2.6] \n",
      "Epoch 11/50: 100%|██████████| 157/157 [02:51<00:00,  1.09s/it, loss=2.42]\n",
      "Epoch 12/50: 100%|██████████| 157/157 [02:54<00:00,  1.11s/it, loss=2.47]\n",
      "Epoch 13/50: 100%|██████████| 157/157 [02:36<00:00,  1.00it/s, loss=2.58]\n",
      "Epoch 14/50: 100%|██████████| 157/157 [02:47<00:00,  1.07s/it, loss=2.06]\n",
      "Epoch 15/50: 100%|██████████| 157/157 [03:11<00:00,  1.22s/it, loss=1.97]\n",
      "Epoch 16/50: 100%|██████████| 157/157 [02:32<00:00,  1.03it/s, loss=1.56]\n",
      "Epoch 17/50: 100%|██████████| 157/157 [02:23<00:00,  1.09it/s, loss=1.82]\n",
      "Epoch 18/50: 100%|██████████| 157/157 [02:10<00:00,  1.21it/s, loss=1.76]\n",
      "Epoch 19/50: 100%|██████████| 157/157 [01:49<00:00,  1.43it/s, loss=1.44]\n",
      "Epoch 20/50: 100%|██████████| 157/157 [01:54<00:00,  1.37it/s, loss=1.47]\n",
      "Epoch 21/50: 100%|██████████| 157/157 [02:53<00:00,  1.11s/it, loss=1.41]\n",
      "Epoch 22/50: 100%|██████████| 157/157 [01:50<00:00,  1.43it/s, loss=1.14]\n",
      "Epoch 23/50: 100%|██████████| 157/157 [01:49<00:00,  1.44it/s, loss=1.08]\n",
      "Epoch 24/50: 100%|██████████| 157/157 [01:56<00:00,  1.35it/s, loss=1.05] \n",
      "Epoch 25/50: 100%|██████████| 157/157 [02:14<00:00,  1.17it/s, loss=1.03] \n",
      "Epoch 26/50: 100%|██████████| 157/157 [02:24<00:00,  1.09it/s, loss=1.05] \n",
      "Epoch 27/50: 100%|██████████| 157/157 [02:37<00:00,  1.00s/it, loss=0.932]\n",
      "Epoch 28/50: 100%|██████████| 157/157 [02:45<00:00,  1.06s/it, loss=0.881]\n",
      "Epoch 29/50: 100%|██████████| 157/157 [02:50<00:00,  1.09s/it, loss=0.808]\n",
      "Epoch 30/50: 100%|██████████| 157/157 [02:43<00:00,  1.04s/it, loss=0.691]\n",
      "Epoch 31/50: 100%|██████████| 157/157 [02:42<00:00,  1.04s/it, loss=0.787]\n",
      "Epoch 32/50: 100%|██████████| 157/157 [02:14<00:00,  1.16it/s, loss=0.796]\n",
      "Epoch 33/50: 100%|██████████| 157/157 [01:53<00:00,  1.39it/s, loss=0.559]\n",
      "Epoch 34/50: 100%|██████████| 157/157 [02:09<00:00,  1.21it/s, loss=0.638]\n",
      "Epoch 35/50: 100%|██████████| 157/157 [01:50<00:00,  1.42it/s, loss=0.552]\n",
      "Epoch 36/50: 100%|██████████| 157/157 [01:49<00:00,  1.43it/s, loss=0.409]\n",
      "Epoch 37/50: 100%|██████████| 157/157 [01:49<00:00,  1.44it/s, loss=0.559]\n",
      "Epoch 38/50: 100%|██████████| 157/157 [01:48<00:00,  1.45it/s, loss=0.469]\n",
      "Epoch 39/50: 100%|██████████| 157/157 [02:01<00:00,  1.30it/s, loss=0.409]\n",
      "Epoch 40/50: 100%|██████████| 157/157 [01:57<00:00,  1.34it/s, loss=0.335]\n",
      "Epoch 41/50: 100%|██████████| 157/157 [01:53<00:00,  1.38it/s, loss=0.274]\n",
      "Epoch 42/50: 100%|██████████| 157/157 [01:50<00:00,  1.42it/s, loss=0.348]\n",
      "Epoch 43/50: 100%|██████████| 157/157 [01:49<00:00,  1.43it/s, loss=0.33] \n",
      "Epoch 44/50: 100%|██████████| 157/157 [02:41<00:00,  1.03s/it, loss=0.337]\n",
      "Epoch 45/50: 100%|██████████| 157/157 [03:18<00:00,  1.26s/it, loss=0.485]\n",
      "Epoch 46/50: 100%|██████████| 157/157 [03:14<00:00,  1.24s/it, loss=0.349]\n",
      "Epoch 47/50: 100%|██████████| 157/157 [03:19<00:00,  1.27s/it, loss=0.342]\n",
      "Epoch 48/50: 100%|██████████| 157/157 [03:16<00:00,  1.25s/it, loss=0.249]\n",
      "Epoch 49/50: 100%|██████████| 157/157 [03:11<00:00,  1.22s/it, loss=0.28] \n",
      "Epoch 50/50: 100%|██████████| 157/157 [03:18<00:00,  1.26s/it, loss=0.294]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: 74.06826939419577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train(model, optimizer, criterion, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model, input_text, input_tokenizer, target_tokenizer, max_length=MAX_LENGTH\n",
    "):\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Convert text to sequence\n",
    "    input_sequence = input_tokenizer.texts_to_sequences([input_text])\n",
    "\n",
    "    # Apply padding\n",
    "    padded_input_sequence = pad_sequences(\n",
    "        input_sequence, maxlen=max_length, padding=\"post\"\n",
    "    )\n",
    "\n",
    "    # Convert to torch tensor\n",
    "    input_tensor = torch.tensor(padded_input_sequence, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(input_tensor)\n",
    "\n",
    "    sos_token = target_tokenizer.word_index[\"sos\"]\n",
    "\n",
    "    # Start prediction with \"sos\" token\n",
    "    x_input = torch.tensor([sos_token], dtype=torch.long)\n",
    "\n",
    "    # List to store predicted tokens\n",
    "    translated_sentence = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            prediction, hidden, cell = model.decoder(x_input, hidden, cell)\n",
    "\n",
    "        predicted_token = prediction.argmax(1).item()\n",
    "\n",
    "        # Stop prediction if \"eos\" is predicted\n",
    "        if predicted_token == target_tokenizer.word_index[\"eos\"]:\n",
    "            break\n",
    "\n",
    "        translated_sentence.append(predicted_token)\n",
    "\n",
    "        # Update x_input for next time step\n",
    "        x_input = torch.tensor([predicted_token], dtype=torch.long)\n",
    "\n",
    "    # Convert tokens back to text\n",
    "    translated_sentence = target_tokenizer.sequences_to_texts([translated_sentence])[0]\n",
    "\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: ask tom instead\n",
      "Translated Sentence: pregunta a tom en su lugar\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "input_sentence = \"ask tom instead\"\n",
    "translated_sentence = predict(model, input_sentence, input_tokenizer, target_tokenizer)\n",
    "print(f\"Input Sentence: {input_sentence}\")\n",
    "print(f\"Translated Sentence: {translated_sentence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
