{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "MAX_LENGTH = 25\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "CLIP = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"eng_spn.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = df[\"English words/sentences\"]\n",
    "target_data = df[\"Spanish words/sentences\"].apply(lambda x: \"<sos> \" + x + \" <eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(input_data)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_data)\n",
    "\n",
    "target_tokenizer = Tokenizer()\n",
    "target_tokenizer.fit_on_texts(target_data)\n",
    "target_sequences = target_tokenizer.texts_to_sequences(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "padded_input_sequences = pad_sequences(\n",
    "    input_sequences, maxlen=MAX_LENGTH, padding=\"post\"\n",
    ")\n",
    "\n",
    "padded_target_sequences = pad_sequences(\n",
    "    target_sequences, maxlen=MAX_LENGTH, padding=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab size\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pytorch tensors\n",
    "input_tensor = torch.tensor(padded_input_sequences, dtype=torch.long)\n",
    "target_tensor = torch.tensor(padded_target_sequences, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dataloader = DataLoader(\n",
    "    TensorDataset(input_tensor, target_tensor), batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert tokens to vectors\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Pass the embedded vector into LSTM layer\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Linear layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # Add batch dimension\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # Convert tokens to vectors\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Pass the embedded vector into LSTM layer\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "\n",
    "        # Remove the batch dimension\n",
    "        lstm_output = lstm_output.squeeze(1)\n",
    "\n",
    "        # Generate predictions for the next token\n",
    "        prediction = self.fc(lstm_output)\n",
    "\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_vocab_size,\n",
    "        target_vocab_size,\n",
    "        embed_dim,\n",
    "        hidden_dim,\n",
    "        num_layers,\n",
    "        dropout,\n",
    "    ):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            input_vocab_size, embed_dim, hidden_dim, num_layers, dropout\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            target_vocab_size, embed_dim, hidden_dim, num_layers, dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        batch_size, max_length = target.size()\n",
    "        target_vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        # Tensor to store outputs for all time steps\n",
    "        outputs = torch.zeros(batch_size, max_length, target_vocab_size)\n",
    "\n",
    "        # Get hidden and cell states from the encoder\n",
    "        hidden, cell = self.encoder(input)\n",
    "\n",
    "        # Start decoding with the first target token\n",
    "        target_input_token = target[:, 0]\n",
    "\n",
    "        for t in range(1, max_length):\n",
    "            decoder_output, hidden, cell = self.decoder(\n",
    "                target_input_token, hidden, cell\n",
    "            )\n",
    "            outputs[:, t, :] = decoder_output\n",
    "            target_input_token = target[:, t]\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Seq2Seq(\n",
    "    input_vocab_size,\n",
    "    target_vocab_size,\n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_LAYERS,\n",
    "    DROPOUT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "\n",
    "\n",
    "# Save model function\n",
    "def save_checkpoint(epoch, model, filename=\"checkpoint.pth\"):\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "        },\n",
    "        filename,\n",
    "    )\n",
    "\n",
    "\n",
    "# Load model function\n",
    "def load_checkpoint(model, filename):\n",
    "    checkpoint = torch.load(filename)\n",
    "\n",
    "    start_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    return start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "try:\n",
    "    start_epoch = load_checkpoint(model, filename=\"checkpoint.pth\")\n",
    "    print(f\"Resuming training from epoch: {start_epoch}\")\n",
    "except FileNotFoundError:\n",
    "    start_epoch = 1\n",
    "    print(f\"No checkpoint found, starting training from scratch...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Adam optimizer and Loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "def train(model, optimizer, criterion, dataloader, epochs=NUM_EPOCHS):\n",
    "\n",
    "    model.train()  # Set model to Training mode\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "        for input, target in progress_bar:\n",
    "\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(input, target)\n",
    "\n",
    "            # Reshape input and target to calculate loss\n",
    "            output = output[:, 1:].reshape(-1, output.shape[2])\n",
    "            target = target[:, 1:].reshape(-1)\n",
    "\n",
    "            # Compute loss and backpropagation\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "\n",
    "            # Update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        total_loss += epoch_loss\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "        save_checkpoint(epoch, model)\n",
    "\n",
    "    print(f\"Total Loss: {total_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "train(model, optimizer, criterion, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU score implementation (token-level)\n",
    "\n",
    "\n",
    "def compute_bleu(reference, candidate, max_n=4, weights=None):\n",
    "    if weights is None:\n",
    "        weights = [1 / max_n] * max_n\n",
    "\n",
    "    # No need to split, as reference and candidate are already tokenized lists\n",
    "    reference_tokens = reference\n",
    "    candidate_tokens = candidate\n",
    "\n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        # Extract n-grams for reference and candidate\n",
    "        ref_ngrams = Counter(\n",
    "            [\n",
    "                tuple(reference_tokens[i : i + n])\n",
    "                for i in range(len(reference_tokens) - n + 1)\n",
    "            ]\n",
    "        )\n",
    "        cand_ngrams = Counter(\n",
    "            [\n",
    "                tuple(candidate_tokens[i : i + n])\n",
    "                for i in range(len(candidate_tokens) - n + 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        match_count = sum(min(ref_ngrams[ng], cand_ngrams[ng]) for ng in cand_ngrams)\n",
    "        total_count = max(len(candidate_tokens) - n + 1, 1)\n",
    "        precisions.append(match_count / total_count if total_count > 0 else 0)\n",
    "\n",
    "    reference_length = len(reference_tokens)\n",
    "    candidate_length = len(candidate_tokens)\n",
    "    brevity_penalty = (\n",
    "        math.exp(1 - reference_length / candidate_length)\n",
    "        if candidate_length < reference_length\n",
    "        else 1\n",
    "    )\n",
    "\n",
    "    bleu_score = brevity_penalty * math.exp(\n",
    "        sum(w * math.log(p) for w, p in zip(weights, precisions) if p > 0)\n",
    "    )\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, compute_bleu, max_n=4, weights=None):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    total_bleu_score = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "        for input, target in dataloader:\n",
    "\n",
    "            # Forward pass (get predictions)\n",
    "            output = model(input, target)\n",
    "\n",
    "            # Reshape output and target for comparison\n",
    "            output = output[:, 1:].argmax(\n",
    "                dim=-1\n",
    "            )  # Predicted tokens (without <start> token)\n",
    "            target = target[:, 1:]  # Ignore <start> token in the reference\n",
    "\n",
    "            # Convert target and predicted tokens to lists (detach from GPU if necessary)\n",
    "            target_tokens = target.cpu().tolist()\n",
    "            predicted_tokens = output.cpu().tolist()\n",
    "\n",
    "            # Calculate BLEU score for the current batch\n",
    "            batch_bleu_score = 0\n",
    "            for ref, pred in zip(target_tokens, predicted_tokens):\n",
    "                batch_bleu_score += compute_bleu(\n",
    "                    reference=ref, candidate=pred, max_n=max_n, weights=weights\n",
    "                )\n",
    "\n",
    "            # Accumulate BLEU scores and total samples\n",
    "            total_bleu_score += batch_bleu_score\n",
    "            total_samples += len(target_tokens)\n",
    "\n",
    "    avg_bleu_score = total_bleu_score / total_samples\n",
    "\n",
    "    return avg_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, dataloader, compute_bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model, input_text, input_tokenizer, target_tokenizer, max_length=MAX_LENGTH\n",
    "):\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Convert text to sequence\n",
    "    input_sequence = input_tokenizer.texts_to_sequences([input_text])\n",
    "\n",
    "    # Apply padding\n",
    "    padded_input_sequence = pad_sequences(\n",
    "        input_sequence, maxlen=max_length, padding=\"post\"\n",
    "    )\n",
    "\n",
    "    # Convert to torch tensor\n",
    "    input_tensor = torch.tensor(padded_input_sequence, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(input_tensor)\n",
    "\n",
    "    sos_token = target_tokenizer.word_index[\"sos\"]\n",
    "\n",
    "    # Start prediction with \"sos\" token\n",
    "    x_input = torch.tensor([sos_token], dtype=torch.long)\n",
    "\n",
    "    # List to store predicted tokens\n",
    "    translated_sentence = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            prediction, hidden, cell = model.decoder(x_input, hidden, cell)\n",
    "\n",
    "        predicted_token = prediction.argmax(1).item()\n",
    "\n",
    "        # Stop prediction if \"eos\" is predicted\n",
    "        if predicted_token == target_tokenizer.word_index[\"eos\"]:\n",
    "            break\n",
    "\n",
    "        translated_sentence.append(predicted_token)\n",
    "\n",
    "        # Update x_input for next time step\n",
    "        x_input = torch.tensor([predicted_token], dtype=torch.long)\n",
    "\n",
    "    # Convert tokens back to text\n",
    "    translated_sentence = target_tokenizer.sequences_to_texts([translated_sentence])[0]\n",
    "\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "input_sentence = \"ask tom instead\"\n",
    "translated_sentence = predict(model, input_sentence, input_tokenizer, target_tokenizer)\n",
    "print(f\"Input Sentence: {input_sentence}\")\n",
    "print(f\"Translated Sentence: {translated_sentence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
